{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7VNgmJuv8kc_",
   "metadata": {
    "id": "7VNgmJuv8kc_"
   },
   "source": [
    "# Install all necessary modules here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "xT0cnjh_Mk_Z",
   "metadata": {
    "id": "xT0cnjh_Mk_Z"
   },
   "outputs": [],
   "source": [
    "# Using dataset from moodle, and now albumentations is now the main form of transformations as it was designed to do so. Testing with torchvision.transformations leads to errors\n",
    "# such as TypeError: Normalize.forward() got an unexpected keyword argument 'image'\n",
    "import torch\n",
    "import numpy as np\n",
    "import platform\n",
    "\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from numpy import random\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, annotations_frame,\n",
    "                 path_to_slides,\n",
    "                 crop_size = (128,128),\n",
    "                 pseudo_epoch_length:int = 1000,\n",
    "                 transformations = None):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        if platform.system() == 'Linux':\n",
    "            self.separator = '/'\n",
    "        else:\n",
    "            self.separator = '\\\\'\n",
    "\n",
    "        self.anno_frame = annotations_frame\n",
    "        self.path_to_slides = path_to_slides\n",
    "        self.crop_size = crop_size\n",
    "        self.pseudo_epoch_length = pseudo_epoch_length\n",
    "        \n",
    "        # list which holds annotations of all slides in slide_names in the format\n",
    "        # slide_name, annotation, label, min_x, max_x, min_y, max_y\n",
    "        \n",
    "        self.slide_dict, self.annotations_list = self._initialize()\n",
    "        self.sample_cord_list = self._sample_cord_list()\n",
    "\n",
    "        # set up transformations\n",
    "        self.transformations = transformations\n",
    "        self.transform_to_tensor = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "\n",
    "    def _initialize(self):\n",
    "        # open all images and store them in self.slide_dict with their name as key value\n",
    "        slide_dict = {}\n",
    "        annotations_list = []\n",
    "        for slide in self.anno_frame.filename.unique():\n",
    "            # open slide\n",
    "            im_obj = Image.open(self.path_to_slides + self.separator + slide).convert('RGB')\n",
    "            slide_dict[slide] = im_obj\n",
    "            # setting up a list with all bounding boxes\n",
    "            for idx,annotations in self.anno_frame[self.anno_frame.filename == slide][['max_x','max_y','min_x','min_y','label']].iterrows():\n",
    "                annotations_list.append([slide, annotations['label'], annotations['min_x'], annotations['min_y'], annotations['max_x'], annotations['max_y']])\n",
    "\n",
    "        return slide_dict, annotations_list\n",
    "\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        slide, x_cord, y_cord = self.sample_cord_list[index]\n",
    "        x_cord = np.int64(x_cord)\n",
    "        y_cord = np.int64(y_cord)\n",
    "        # load image\n",
    "        img = self.slide_dict[slide].crop((x_cord,y_cord,x_cord + self.crop_size[0],y_cord + self.crop_size[1]))\n",
    "        # transform image\n",
    "        #img = self.transformations(img)\n",
    "        \n",
    "        # load boxes for the image\n",
    "        labels_boxes = self._get_boxes_and_label(slide,x_cord,y_cord)\n",
    "        \n",
    "        labels_boxes = [[i[1] - x_cord, i[2] - y_cord, i[3] - x_cord, i[4] - y_cord] + [i[0]] for i in labels_boxes]\n",
    "        \n",
    "        \n",
    "        # applay transformations\n",
    "        if self.transformations != None:\n",
    "            if len(labels_boxes) > 0:\n",
    "                transformed = self.transformations(image = np.array(img), bboxes = labels_boxes)\n",
    "                boxes = torch.tensor([line[:-1] for line in transformed['bboxes']], dtype = torch.float32)\n",
    "                labels = torch.ones(boxes.shape[0], dtype = torch.int64)\n",
    "                img = self.transform_to_tensor(transformed['image'])\n",
    "                \n",
    "            # check if there is no labeld instance on the image\n",
    "            if len(labels_boxes) == 0:\n",
    "                labels = torch.tensor([0], dtype = torch.int64)\n",
    "                boxes = torch.zeros((0,4),dtype = torch.float32)\n",
    "                img = self.transform_to_tensor(img)\n",
    "\n",
    "        else:\n",
    "            if len(labels_boxes) == 0:\n",
    "                labels = torch.tensor([0], dtype = torch.int64)\n",
    "                boxes = torch.zeros((0,4),dtype = torch.float32)\n",
    "                img = self.transform_to_tensor(img)\n",
    "            else:\n",
    "                # now, you need to change the originale box cordinates to the cordinates of the image\n",
    "                boxes = torch.tensor([line[:-1] for line in labels_boxes],dtype=torch.float32)\n",
    "                labels = torch.ones(boxes.shape[0], dtype = torch.int64)\n",
    "                img = self.transform_to_tensor(img)\n",
    "\n",
    "        target = {\n",
    "            'boxes': boxes,\n",
    "            'labels':labels\n",
    "        }\n",
    "\n",
    "        return img, target\n",
    "        \n",
    "\n",
    "    def _sample_cord_list(self):\n",
    "        # select slides from which to sample an image\n",
    "        slide_names = np.array(list(self.slide_dict.keys()))\n",
    "        slide_indice = random.choice(np.arange(len(slide_names)), size = self.pseudo_epoch_length, replace = True)\n",
    "        slides = slide_names[slide_indice]\n",
    "        # select coordinates from which to load images\n",
    "        # only works if all images have the same size\n",
    "        width,height = self.slide_dict[slides[0]].size\n",
    "        cordinates = random.randint(low = (0,0), high=(width - self.crop_size[0], height - self.crop_size[1]), size = (self.pseudo_epoch_length,2))\n",
    "        return np.concatenate((slides.reshape(-1,1),cordinates), axis = -1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.pseudo_epoch_length\n",
    "\n",
    "    def _get_boxes_and_label(self,slide,x_cord,y_cord):\n",
    "        return [line[1::] for line in self.annotations_list if line[0] == slide and line[2] > x_cord and line [3] > y_cord and line[4] < x_cord + self.crop_size[0] and line[5] < y_cord + self.crop_size[1]]\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        \"\"\"\n",
    "        Since each image may have a different number of objects, we need a collate function (to be passed to the DataLoader).\n",
    "        This describes how to combine these tensors of different sizes. We use lists.\n",
    "        Note: this need not be defined in this Class, can be standalone.\n",
    "        :param batch: an iterable of N sets from __iter__()\n",
    "        :return: a tensor of images, lists of varying-size tensors of bounding boxes, labels, and difficulties\n",
    "        \"\"\"\n",
    "\n",
    "        images = list()\n",
    "        targets = list()\n",
    "\n",
    "        for b in batch:\n",
    "            images.append(b[0])\n",
    "            targets.append(b[1])\n",
    "            \n",
    "        images = torch.stack(images, dim=0)\n",
    "\n",
    "        return images, targets\n",
    "\n",
    "    def trigger_sampling(self):\n",
    "        self.sample_cord_list = self._sample_cord_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "KvENQXLNn05-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KvENQXLNn05-",
    "outputId": "9940a58c-424f-4552-9229-1bedc1b45a12"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\eujin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\eujin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: torchmetrics in c:\\users\\eujin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (0.11.4)\n",
      "Requirement already satisfied: numpy>=1.17.2 in c:\\users\\eujin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torchmetrics) (1.23.5)\n",
      "Requirement already satisfied: torch>=1.8.1 in c:\\users\\eujin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torchmetrics) (2.0.1+cu118)\n",
      "Requirement already satisfied: packaging in c:\\users\\eujin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torchmetrics) (21.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\eujin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch>=1.8.1->torchmetrics) (3.4.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\eujin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch>=1.8.1->torchmetrics) (4.2.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\eujin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch>=1.8.1->torchmetrics) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\eujin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch>=1.8.1->torchmetrics) (2.8)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\eujin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch>=1.8.1->torchmetrics) (3.1.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\eujin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from packaging->torchmetrics) (2.4.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\eujin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from jinja2->torch>=1.8.1->torchmetrics) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\eujin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sympy->torch>=1.8.1->torchmetrics) (1.2.1)\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-4c4b585c7136>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'pip install torchmetrics'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdrive\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/content/gdrive'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "!pip install torchmetrics\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906z5qHw2Cit",
   "metadata": {
    "id": "906z5qHw2Cit"
   },
   "outputs": [],
   "source": [
    "\n",
    "import pickle\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "import albumentations as A\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LBQjJgJWZYvU",
   "metadata": {
    "id": "LBQjJgJWZYvU"
   },
   "outputs": [],
   "source": [
    "# load annotation data and set path to images\n",
    "anno_frame = pickle.load(open('/content/gdrive/MyDrive/AgNORs/annotation_frame.p','rb'))\n",
    "path_to_slides = '/content/gdrive/MyDrive/AgNORs/'\n",
    "transform1 = A.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
    "#anno_data = pickle.load(open(annotation_frame,'rb'))\n",
    "\n",
    "# instantiate your dataset, you have to define or import it first\n",
    "ds = Dataset(annotations_frame=anno_frame,\n",
    "        path_to_slides=path_to_slides,\n",
    "        crop_size=(512,512),\n",
    "        transformations= transform1)\n",
    "        #transformations=None)\n",
    "#img,target = ds[0]\n",
    "#print(target)\n",
    "#plt.imshow(img.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bf9f74",
   "metadata": {
    "id": "d3bf9f74"
   },
   "source": [
    "# Preparation of milestone three\n",
    "\n",
    "Today we will start preparing the third milestone. The third milestone is to train an object detector to recognize cells. To successfully complete the milestone, you will have to complete the following sub-tasks:\n",
    "- Initialize a pytorch object detector. I'd suggest to choose a RetinaNet or FCOS detection model. More Information can be found [here](https://pytorch.org/vision/stable/models.html#object-detection-instance-segmentation-and-person-keypoint-detection). Since we do not have endless compute power available, we will use a frozen ResNet18 pre-trained on ImageNet as backbone and only train the detection and classification heads of our object detector. So you will have to finde a way to **freeze** the backbone of your detector.\n",
    "- You will have to write a [training and validation/test](https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html) loop to train your detector. Make sure you measure the convergence of the training by monitoring a detection metric like the [mAP](https://torchmetrics.readthedocs.io/en/stable/detection/mean_average_precision.html). Also, you will have to find a way to select the best model during training based on some metric.\n",
    "- You will have to train your model until convergence using the  class you created for the last milestone. Also you will have to pass your dataset to a dataloader to be able to use multithreading as well as automatic batching.\n",
    "- At the end, you will have to save the **state_dict** of your trained object detector, to be able to reuse it later.\n",
    "\n",
    "Please use a jupyter notebook for coding your training/testing pipeline. In the end, you will have to submit that jupiter notebook at moodle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418e17a1",
   "metadata": {
    "id": "418e17a1"
   },
   "source": [
    "# If you run the notebook in colab, you have to mount the google drive with the images. Proceed as follows:\n",
    "\n",
    "- **First**: Open the following **[link](https://drive.google.com/drive/folders/18P74V8kli6qDZtGBLN-tPrJFu3O2NPEK?usp=sharing)** in a new tab.\n",
    "- **Second**: Add a link to your google Drive.\n",
    "Example: [Link](https://drive.google.com/file/d/1IcFGGIoktPkDj9-4j5IQ3evInn0c2aq-/view?usp=sharing)\n",
    "- **Third**: Run the line of code below\n",
    "- **Fourth**: Grant Google access to your Drive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9CkOa6zxbj5Z",
   "metadata": {
    "id": "9CkOa6zxbj5Z"
   },
   "source": [
    "# 1. Initializing the model\n",
    "\n",
    "In this project, we will use a pre-trained RetinaNet model as the backbone for our object detection task. The model and weights can be easily loaded from the torchvision library. It is important to note that the anchor boxes used by the model may need to be adjusted to suit the specific task.\n",
    "\n",
    "The behavior of the RetinaNet model changes depending on whether it is in training or evaluation mode. During training, the model expects both an image and a dictionary of targets as input. It returns a dictionary containing the losses and predictions. During validation, the model only expects images as input and returns the predictions without calculating any losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2XNNdz0sPf8K",
   "metadata": {
    "id": "2XNNdz0sPf8K"
   },
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "import torch\n",
    "import torchvision\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "mobilenet = torchvision.models.mobilenet_v2()\n",
    "#summary(mobilenet.to(device),(3,512,512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_YC7EQDffb92",
   "metadata": {
    "id": "_YC7EQDffb92"
   },
   "outputs": [],
   "source": [
    "from torchvision.models.detection import RetinaNet\n",
    "from torchvision.models.detection.anchor_utils import AnchorGenerator\n",
    "from torchvision.models import MobileNet_V2_Weights\n",
    "\n",
    "# load a pre-trained model for classification and return\n",
    "# only the features\n",
    "backbone = torchvision.models.mobilenet_v2(weights=MobileNet_V2_Weights.DEFAULT).features\n",
    "# RetinaNet needs to know the number of\n",
    "# output channels in a backbone. For mobilenet_v2, it's 1280,\n",
    "# so we need to add it here\n",
    "backbone.out_channels = 1280\n",
    "# let's make the network generate 5 x 3 anchors per spatial\n",
    "# location, with 5 different sizes and 3 different aspect\n",
    "# ratios. We have a Tuple[Tuple[int]] because each feature\n",
    "# map could potentially have different sizes and\n",
    "# aspect ratios\n",
    "anchor_generator = AnchorGenerator(\n",
    "     sizes=((32, 64, 128, 256, 512),),\n",
    "     aspect_ratios=((0.5, 1.0, 2.0),)\n",
    ")\n",
    "# put the pieces together inside a RetinaNet model\n",
    "model = RetinaNet(backbone,\n",
    "                  num_classes=2,\n",
    "                  anchor_generator=anchor_generator)\n",
    "\n",
    "######## uncomment these lines to freeze you network ############\n",
    "for p in model.backbone.parameters():\n",
    "  p.requires_grad = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ryYVXb8Hf-",
   "metadata": {
    "id": "c0ryYVXb8Hf-"
   },
   "source": [
    "# 2. Setting up an optimzer, a detection metric and the train and validation dataloaders\n",
    "\n",
    "To train the object detector, it is necessary to select an appropriate optimizer. Additionally, the torchmetrics class needs to be instantiated before it can be used for evaluation or tracking metrics during training.\n",
    "Additionally, initialize a training and validation dataloader your dataset. For more information on how to set up your dataloaders have a look [here](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4iNlBnCU8pX3",
   "metadata": {
    "id": "4iNlBnCU8pX3"
   },
   "outputs": [],
   "source": [
    "# add your code\n",
    "from torch.optim import Adam\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=0.01, betas=(0.9,0.999))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "I2s7huwv8s72",
   "metadata": {
    "id": "I2s7huwv8s72"
   },
   "source": [
    "#3. Train and validation loop\n",
    "\n",
    "Please write two functions, one for training and one for evaluating your object detector. Use these functions to train the detector for a few epochs. During training, track both the training losses and validation metrics to monitor the model's performance. Save the best detector as observerd by the validation metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DniR_K0vmOnl",
   "metadata": {
    "id": "DniR_K0vmOnl"
   },
   "outputs": [],
   "source": [
    "def train(dataloader, model, optimizer):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"The model will be running on\", device, \"device\")\n",
    "\n",
    "    running_loss = 0\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    # switch to train mode\n",
    "    if not model.training:\n",
    "        model.train()\n",
    "    size = len(dataloader.dataset)\n",
    "    for epoch in range(5):\n",
    "        for batch, (images, labels) in enumerate(dataloader):\n",
    "            # Compute prediction and loss\n",
    "            optimizer.zero_grad()\n",
    "            images = images.to(device)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in labels]\n",
    "\n",
    "            # labels = labels.to(device)\n",
    "            loss_dict = model(images, targets)\n",
    "            totalLoss = sum(loss for loss in loss_dict.values())\n",
    "            running_loss += totalLoss\n",
    "\n",
    "            # Backpropagation\n",
    "            totalLoss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # print your losses here\n",
    "    print(running_loss/dataloader.batch_size)\n",
    "\n",
    "\n",
    "def validation(dataloader,model):\n",
    "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "  metric= MeanAveragePrecision()\n",
    "  running_loss = 0.0\n",
    "\n",
    "  loss_classifier = 0.0\n",
    "  loss_box_reg = 0.0\n",
    "  \n",
    "  preds = []\n",
    "  tag = []\n",
    "\n",
    "  # switch to validation mode\n",
    "  if model.training:\n",
    "      model.eval()\n",
    "\n",
    "  with torch.no_grad():\n",
    "          # iterating over batches in valoader_loader\n",
    "      for i, (images,targets) in enumerate(dataloader):\n",
    "          images = images.to(device)\n",
    "\n",
    "          targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "          predictions = model(images)\n",
    "          \n",
    "          # handle images with no detections\n",
    "          for idx,t in enumerate(targets):\n",
    "              if len(t['boxes']) == 0:\n",
    "                  targets[idx]['boxes'] = torch.tensor([[0,0,0,0]], dtype = torch.float32).to(device)\n",
    "          \n",
    "          #return predictions, targets\n",
    "          metric.update(predictions,targets)\n",
    "          preds.append(predictions)\n",
    "          tag.append(targets)\n",
    "          \n",
    "  #return preds, tag\n",
    "  metrics_values = metric.compute()\n",
    "  \n",
    "  return  metrics_values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "x7ZxsmMnJgdk",
   "metadata": {
    "id": "x7ZxsmMnJgdk"
   },
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "num_workers = 2\n",
    "num_samples = 25 \n",
    "transform1 =A.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
    "# train val split\n",
    "imgs = anno_frame.filename.unique()\n",
    "train_imgs = imgs[np.random.choice(np.arange(len(imgs)),size = int(0.75 * len(imgs)), replace = False)]\n",
    "val_imgs = [i for i in imgs if i not in train_imgs]\n",
    "train_df = anno_frame[anno_frame.filename.isin(train_imgs)]\n",
    "val_df = anno_frame[anno_frame.filename.isin(val_imgs)]\n",
    "\n",
    "train_ds = Dataset(annotations_frame = train_df,\n",
    "                   path_to_slides = path_to_slides,\n",
    "                   crop_size = (256,256),\n",
    "                   pseudo_epoch_length=num_samples,\n",
    "                   transformations=transform1)\n",
    "val_ds = Dataset(annotations_frame = val_df,\n",
    "                   path_to_slides = path_to_slides,\n",
    "                   crop_size = (256,256),\n",
    "                 pseudo_epoch_length=num_samples,\n",
    "                 transformations = transform1)\n",
    "\n",
    "#NOW TO CONVERT INTO DATALOADERS\n",
    "train_dl = DataLoader(train_ds,\n",
    "                      batch_size = batch_size,\n",
    "                      num_workers = num_workers,\n",
    "                      collate_fn = train_ds.collate_fn)\n",
    "val_dl = DataLoader(val_ds,\n",
    "                    batch_size = batch_size,\n",
    "                    num_workers = num_workers,\n",
    "                    collate_fn = val_ds.collate_fn)\n",
    "\n",
    "# free some space\n",
    "del val_ds\n",
    "del train_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ivwojB899YXw",
   "metadata": {
    "id": "ivwojB899YXw"
   },
   "source": [
    "#4. Show some results and save your detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LSfIMQu0mRGd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 432
    },
    "id": "LSfIMQu0mRGd",
    "outputId": "86312ff9-3c2e-496d-8778-145f597a5290"
   },
   "outputs": [],
   "source": [
    "max_epochs = 15\n",
    "losses_train = []\n",
    "mAP_val = []\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "best_map = 0.0\n",
    "\n",
    "for e in range(max_epochs):\n",
    "    print(f\"Epoch {e+1}\")\n",
    "\n",
    "    # resample list\n",
    "    train_dl.dataset.trigger_sampling()\n",
    "    # training loop\n",
    "    train_loss = train(\n",
    "    dataloader = train_dl,\n",
    "    model = model,\n",
    "    optimizer = optimizer,\n",
    "    )\n",
    "\n",
    "    losses_train.append(train_loss)\n",
    "\n",
    "    # validation loop\n",
    "    metrics = validation(\n",
    "        model = model,\n",
    "        dataloader = val_dl)\n",
    "\n",
    "    mAP_val.append(metrics['map_50'].numpy())\n",
    "    \n",
    "    if metrics['map_50'] > best_map:\n",
    "        print(f\"Saving best model at epoch {e+1} with mAP50 of {metrics['map_50'].numpy():.3f}\")\n",
    "        best_model = model.state_dict()\n",
    "        best_map = metrics['map_50']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7p87FDwGpYoL",
   "metadata": {
    "id": "7p87FDwGpYoL"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "interpreter": {
   "hash": "cbc2ccb7899380f8e39344ba6fff5f38db9a0526ef90eff1147cd398f3029ddf"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

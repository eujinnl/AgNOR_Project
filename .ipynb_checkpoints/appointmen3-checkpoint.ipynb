{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course number 3, Task 1 Pytorch Datasets\n",
    "\n",
    "To train neural networks in pytorch with custom datasets, custom dataset classes are needed. Today we will work on a dataset for the term project. A general introduction to custom datasets can be found [here](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html) and [here](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html).\n",
    "\n",
    "Your dataset must have several properties that are specific to the task:\n",
    "1. usually, when working with histopathology images, we work with image patches instead of whole images because they are too large to fit in the gpu-memory. These image patches are randomly sampled either on the fly or at the beginning of each epoch. For this reason, your dataset needs a \"crop_size\" parameter that determines the size of the patches, and a \"pseudo_epoch_length\" parameter that determines how many patches are sampled from the training images in each epoch.\n",
    "2. since you have only a small number of images available, you must use image augmentation. Image augmentation is always applied on the fly. **Never apply image augmentation to images and save the augmented versions to disk for sampling!** You can use the image augmentations implemented in the [albumentations](https://albumentations.ai/docs/getting_started/bounding_boxes_augmentation/) lybrary.\n",
    "3. don't open the images each time you want to sample a patch. Instead, write an initialization method which opens each image and stores the image object in a list or dictionary. Then only access the part of the image you want to return throught that image object.\n",
    "4. path_to_slieds is the path where you stored the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from PIL import Image # You can use Image to open the images. Image objects also have a crop method that might be useful\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, annotations_frame, path_to_slides, crop_size = (128,128), pseudo_epoch_length:int = 1000, transformations = None):\n",
    "        super().__init__()\n",
    "        pass\n",
    "        # Initialization of images\n",
    "        # Initialization of coordinates from which to sample patches\n",
    "\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        # Select coordinates from which a patch is extracted\n",
    "        # Gather all coordinates within that patch\n",
    "        # Applay image augmentations on that patch\n",
    "        # Return image, boxes and target\n",
    "        # Use a dictionary to return the targets. It must contain the fileds 'boxes' and 'labels'\n",
    "        \n",
    "        pass\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.pseudo_epoch_length\n",
    "\n",
    "    def initialize(self):\n",
    "        # Open all images and store them in some kind of data structure\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "\n",
    "1.  Initialize your dataset\n",
    "2.  Return some images and targets\n",
    "3.  Visualize the images and the bounding boxes present on them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_images = ''\n",
    "anno_data = pickle.load(open('annotation_frame.p','rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A # Can be used for image augmentations\n",
    "from matplotlib import patches as patches # Can be used for displaying bounding boxes on the images\n",
    "\n",
    "transform = None\n",
    "\n",
    "# Initialize dataset\n",
    "ds = Dataset(annotations_frame=anno_data,\n",
    "        path_to_slides=path_to_images,\n",
    "        crop_size=(512,512),\n",
    "        transformations=transform)\n",
    "\n",
    "# Display some patches"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

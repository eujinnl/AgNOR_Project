{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7VNgmJuv8kc_",
   "metadata": {
    "id": "7VNgmJuv8kc_"
   },
   "source": [
    "# Install all necessary modules here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "XFv3KDvFsxph",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "XFv3KDvFsxph",
    "outputId": "f674320c-ef86-4df4-d6cf-ddce091c7b4f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!pip install torchvision\\n!pip install torchmetrics\\n!pip install torch\\n!pip install numpy'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''!pip install torchvision\n",
    "!pip install torchmetrics\n",
    "!pip install torch\n",
    "!pip install numpy'''"
   ]
  },
  {
   "cell_type": "raw",
   "id": "pQHZuI2Tsx2o",
   "metadata": {
    "id": "pQHZuI2Tsx2o",
    "outputId": "48b6fbdb-00cc-4c80-d44e-b61f3c3b00a6"
   },
   "source": [
    "!pip install torchvision\n",
    "!pip install torchmetrics\n",
    "!pip install torch\n",
    "!pip install numpy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "uo9DV6SrYt4j",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uo9DV6SrYt4j",
    "outputId": "4c7691a0-ea2c-450f-efe5-3bbfb14b636e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: torchmetrics in c:\\users\\eujin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (0.11.4)\n",
      "Requirement already satisfied: numpy>=1.17.2 in c:\\users\\eujin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torchmetrics) (1.23.5)\n",
      "Requirement already satisfied: torch>=1.8.1 in c:\\users\\eujin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torchmetrics) (2.0.1+cu118)\n",
      "Requirement already satisfied: packaging in c:\\users\\eujin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torchmetrics) (21.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\eujin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch>=1.8.1->torchmetrics) (3.4.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\eujin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch>=1.8.1->torchmetrics) (4.2.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\eujin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch>=1.8.1->torchmetrics) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\eujin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch>=1.8.1->torchmetrics) (2.8)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\eujin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch>=1.8.1->torchmetrics) (3.1.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\eujin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from packaging->torchmetrics) (2.4.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\eujin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from jinja2->torch>=1.8.1->torchmetrics) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\eujin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sympy->torch>=1.8.1->torchmetrics) (1.2.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\eujin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\eujin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchmetrics\n",
    "from tqdm.notebook import tqdm\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import platform\n",
    "import pickle\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from numpy import random\n",
    "from torch.utils.data import DataLoader\n",
    "import albumentations as A\n",
    "from matplotlib import pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import torchmetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bf9f74",
   "metadata": {
    "id": "d3bf9f74"
   },
   "source": [
    "# Milestone Four Preparation\n",
    "\n",
    "Today, we will begin preparing for the fourth milestone. The objective of this milestone is to train an object classification model capable of classifying the discovered cells. To successfully complete this milestone, you should follow the steps outlined below:\n",
    "\n",
    "1. Implement a Dataset Class:\n",
    "   - Create a Dataset Class specifically designed to load images from the \"crops\" directory along with their respective labels.\n",
    "   - Considering the significant class imbalance within the dataset, you will have to make use of approprriate sampling and augmentations.\n",
    "   - Resize all crops to the same size.\n",
    "\n",
    "2. Develop a Custom Classification Model:\n",
    "   - Instead of utilizing an existing model from PyTorch, design your own custom model for this task.\n",
    "\n",
    "3. Set up Training and Validation/Test Loops:\n",
    "   - Write a training and validation/test loop for training your model.\n",
    "   - Select an appropriate optimizer and loss function to facilitate effective training.\n",
    "   - Continue training your model until convergence is observed, as indicated by the validation loss.\n",
    "   - Once training is complete, plot the training and test loss to visualize the learning progress.\n",
    "\n",
    "4. Evaluate Model Performance:\n",
    "   - Calculate the f1-score and accuracy of your model.\n",
    "\n",
    "In the end please upload your jupyter notebook to moodle.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418e17a1",
   "metadata": {
    "id": "418e17a1"
   },
   "source": [
    "# If you run the notebook in colab, you have to mount the google drive with the images. Proceed as follows:\n",
    "\n",
    "- **First**: Open the following **[link](https://drive.google.com/drive/folders/1eCU34ZatAXQwzkzHMpV4i2gwlR6qvqpC?usp=share_link)** in a new tab.\n",
    "- **Second**: Add a link to your google Drive.\n",
    "Example: [Link](https://drive.google.com/file/d/1IcFGGIoktPkDj9-4j5IQ3evInn0c2aq-/view?usp=sharing)\n",
    "- **Third**: Run the line of code below\n",
    "- **Fourth**: Grant Google access to your Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6WoxIKCNs6w8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6WoxIKCNs6w8",
    "outputId": "5c7af97e-0e63-4ea9-fafe-65de5fc5ef92"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-898891be04d1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# path to the link you created\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mpath_to_slides\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'/content/gdrive/MyDrive/AgNORs/'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# mount the data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "# path to the link you created\n",
    "path_to_slides = '/content/gdrive/MyDrive/AgNORs/'\n",
    "# mount the data\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "latin-lindsay",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_slides = 'images'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ivwojB899YXw",
   "metadata": {
    "id": "ivwojB899YXw"
   },
   "source": [
    "#1. Implement a Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "gorgeous-responsibility",
   "metadata": {
    "id": "gorgeous-responsibility"
   },
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, annotations_frame, path_to_slides,  num_samples=1000,crop_size=(128,128), transformations= None):\n",
    "        super().__init__()\n",
    "        self.anno_frame = annotations_frame\n",
    "        self.path_to_slides = path_to_slides\n",
    "        self.crop_size = crop_size\n",
    "        self.num_samples = num_samples\n",
    "        self.transformations = transformations\n",
    "        #self.df = pickle.load(open(self.annotations_frame, 'rb'))\n",
    "        self.images = {}\n",
    "        self._initialize()\n",
    "        self.sample_cord_list = self._sample_cord_list()\n",
    "\n",
    "        \n",
    "    def _initialize(self):\n",
    "        for filename in self.anno_frame['filename'].unique():\n",
    "            img_path = f'{self.path_to_slides}/{filename}'\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "            self.images[filename] = img\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        slide=self.sampled[idx][0]\n",
    "        max_x,max_y,min_x,min_y = self.sampled[idx][1:5]\n",
    "        init_img=self.images[slide].crop((min_x,min_y,max_x,max_y))\n",
    "        label = self.sampled[idx][5]\n",
    "        img = A.Compose([\n",
    "            #A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "            A.Resize(30,30),\n",
    "            ToTensorV2()\n",
    "            ])(image=np.asarray(init_img,dtype=np.float32))\n",
    "        return img['image'],label\n",
    "        \n",
    "    def _sample_cord_list(self):\n",
    "        #stratified sampling and so we need to make a dict of available choices with weights(inverse to frequency)\n",
    "        weights = 1/self.anno_frame['label'].value_counts(normalize=True)\n",
    "        hi=dict(weights)\n",
    "        self.anno_frame['weights'] = self.anno_frame.apply(lambda row:hi[row.label],\n",
    "                                                          axis=1)\n",
    "        selected=self.anno_frame.loc[self.anno_frame['label']>0]\n",
    "        sampled=selected.sample(n=num_samples,weights = 'weights').to_numpy()\n",
    "        self.sampled=sampled \n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def trigger_sampling(self):\n",
    "        self.sample_cord_list = self._sample_cord_list()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WeAfxZSy6N6N",
   "metadata": {
    "id": "WeAfxZSy6N6N"
   },
   "source": [
    "#2. Develop a Custom Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "regional-acting",
   "metadata": {
    "id": "regional-acting"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1= nn.Conv2d(3,32,kernel_size = (3,3),stride = 1, padding = 1)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.drop1 = nn.Dropout(0.3)\n",
    "        \n",
    "        self.conv2= nn.Conv2d(32,32,kernel_size = (3,3),stride = 1, padding = 1)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size = (2,2))\n",
    "\n",
    "        self.conv3= nn.Conv2d(32,32,kernel_size = (3,3),stride = 1, padding = 1)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.pool3 = nn.AvgPool2d(kernel_size=(2,2))\n",
    "\n",
    "        self.flat = nn.Flatten()\n",
    "\n",
    "        self.fc3 = nn.Linear(1568,512)\n",
    "        self.act3 = nn.ReLU()\n",
    "        self.drop3 = nn.Dropout(0.3)\n",
    "\n",
    "        self.fc4 = nn.Linear(512,12)\n",
    "        #self.softmax = nn.Softmax(dim=0)\n",
    "\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.act1(self.conv1(x))\n",
    "        x = self.drop1(x)\n",
    "        x = self.act2(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = self.act3(self.conv3(x))\n",
    "        x = self.pool3(x)\n",
    "        x = self.flat(x)\n",
    "        x = self.act3(self.fc3(x))\n",
    "        x = self.drop3(x)\n",
    "        x = self.fc4(x)\n",
    "       # x= self.softmax(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Vl6b7v3_6Ta2",
   "metadata": {
    "id": "Vl6b7v3_6Ta2"
   },
   "source": [
    "#3. Set up Training and Validation/Test Loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "innocent-aggregate",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "innocent-aggregate"
   },
   "outputs": [],
   "source": [
    "def train(dataloader, model, optimizer,criterion):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    #print(\"The model will be running on\", device, \"device\")\n",
    "\n",
    "    running_loss = 0\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    # switch to train mode\n",
    "    if not model.training:\n",
    "        model.train()\n",
    "    size = len(dataloader.dataset)\n",
    "    for epoch in range(5):\n",
    "        for i, data in enumerate(dataloader, 0):\n",
    "        # get the inputs; data is a list of [image, labels]\n",
    "            inputs, labels = data\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            '''if i % 1000 == 0:    # print every 1000 mini-batches/samples\n",
    "                print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "                running_loss = 0.0'''\n",
    "        #print(running_loss/len(dataloader))\n",
    "        running_loss=0.0\n",
    "\n",
    "def validate(dataloader,model):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    running_loss = 0.0\n",
    "\n",
    "    metric = torchmetrics.classification.MulticlassF1Score(num_classes=12)\n",
    "\n",
    "  \n",
    "    #preds = []\n",
    "    #tag = []\n",
    "\n",
    "    # switch to validation mode\n",
    "    if model.training:\n",
    "        model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "          # iterating over batches in valoader_loader\n",
    "        for i, (inputs,targets) in enumerate(dataloader):\n",
    "            predictions = model(inputs)\n",
    "            metric.update(predictions,targets)\n",
    "            #preds.append(predictions)\n",
    "            #tag.append(targets)\n",
    "    #print(preds)\n",
    "    #print(f\"\\n{tag}\")        \n",
    "    #return preds, tag\n",
    "    metrics_values = metric.compute()\n",
    "\n",
    "    print(f\"F1 SCORE = {metrics_values}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "innocent-anxiety",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "innocent-anxiety"
   },
   "outputs": [],
   "source": [
    "#anno_frame = pickle.load(open('/content/gdrive/MyDrive/AgNORs/annotation_frame.p','rb'))\n",
    "anno_frame = pickle.load(open('annotation_frame.p','rb'))\n",
    "num_samples=1000\n",
    "batch_size=25\n",
    "num_workers=2\n",
    "ds = Dataset(annotations_frame = anno_frame, path_to_slides =path_to_slides,num_samples=num_samples)\n",
    "#img,label = ds[4]\n",
    "#plt.imshow(img.permute(1, 2, 0))\n",
    "train_size = int(0.8 * len(ds))\n",
    "test_size = len(ds) - train_size\n",
    "train_ds, val_ds = torch.utils.data.random_split(ds, [train_size, test_size])\n",
    "\n",
    "#NOW TO CONVERT INTO DATALOADERS\n",
    "train_dl = DataLoader(train_ds,\n",
    "                      batch_size = batch_size,\n",
    "                      num_workers = num_workers)\n",
    "val_dl = DataLoader(val_ds,\n",
    "                    batch_size = batch_size,\n",
    "                    num_workers = num_workers)\n",
    "\n",
    "del train_ds\n",
    "del val_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xIrT1Qsg6ben",
   "metadata": {
    "id": "xIrT1Qsg6ben"
   },
   "source": [
    "#4. Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "golden-delta",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "golden-delta",
    "outputId": "2357231a-a934-4170-a9da-3e89310e13ad"
   },
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "model =Net()\n",
    "optimizer = Adam(model.parameters(), lr=0.0001)\n",
    "loss=nn.CrossEntropyLoss()\n",
    "epoch = 20\n",
    "for i in range(epoch):\n",
    "    train_dl.dataset.dataset.trigger_sampling()\n",
    "    train(train_dl,model,optimizer,loss)\n",
    "    validate(val_dl,model)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bqVomcwZohqv",
   "metadata": {
    "id": "bqVomcwZohqv"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "interpreter": {
   "hash": "cbc2ccb7899380f8e39344ba6fff5f38db9a0526ef90eff1147cd398f3029ddf"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7VNgmJuv8kc_",
   "metadata": {
    "id": "7VNgmJuv8kc_"
   },
   "source": [
    "# Install all necessary modules here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XFv3KDvFsxph",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "XFv3KDvFsxph",
    "outputId": "f674320c-ef86-4df4-d6cf-ddce091c7b4f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'!pip install torchvision\\n!pip install torchmetrics\\n!pip install torch\\n!pip install numpy'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''!pip install torchvision\n",
    "!pip install torchmetrics\n",
    "!pip install torch\n",
    "!pip install numpy'''"
   ]
  },
  {
   "cell_type": "raw",
   "id": "pQHZuI2Tsx2o",
   "metadata": {
    "id": "pQHZuI2Tsx2o",
    "outputId": "48b6fbdb-00cc-4c80-d44e-b61f3c3b00a6"
   },
   "source": [
    "!pip install torchvision\n",
    "!pip install torchmetrics\n",
    "!pip install torch\n",
    "!pip install numpy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uo9DV6SrYt4j",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uo9DV6SrYt4j",
    "outputId": "4c7691a0-ea2c-450f-efe5-3bbfb14b636e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.10/dist-packages (0.11.4)\n",
      "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.22.4)\n",
      "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.0.1+cu118)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (23.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.12.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (4.5.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (1.11.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.1.2)\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (2.0.0)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.1->torchmetrics) (3.25.2)\n",
      "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.1->torchmetrics) (16.0.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.1->torchmetrics) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.1->torchmetrics) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchmetrics\n",
    "from tqdm.notebook import tqdm\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import platform\n",
    "import pickle\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from numpy import random\n",
    "from torch.utils.data import DataLoader\n",
    "import albumentations as A\n",
    "from matplotlib import pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import torchmetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bf9f74",
   "metadata": {
    "id": "d3bf9f74"
   },
   "source": [
    "# Milestone Four Preparation\n",
    "\n",
    "Today, we will begin preparing for the fourth milestone. The objective of this milestone is to train an object classification model capable of classifying the discovered cells. To successfully complete this milestone, you should follow the steps outlined below:\n",
    "\n",
    "1. Implement a Dataset Class:\n",
    "   - Create a Dataset Class specifically designed to load images from the \"crops\" directory along with their respective labels.\n",
    "   - Considering the significant class imbalance within the dataset, you will have to make use of approprriate sampling and augmentations.\n",
    "   - Resize all crops to the same size.\n",
    "\n",
    "2. Develop a Custom Classification Model:\n",
    "   - Instead of utilizing an existing model from PyTorch, design your own custom model for this task.\n",
    "\n",
    "3. Set up Training and Validation/Test Loops:\n",
    "   - Write a training and validation/test loop for training your model.\n",
    "   - Select an appropriate optimizer and loss function to facilitate effective training.\n",
    "   - Continue training your model until convergence is observed, as indicated by the validation loss.\n",
    "   - Once training is complete, plot the training and test loss to visualize the learning progress.\n",
    "\n",
    "4. Evaluate Model Performance:\n",
    "   - Calculate the f1-score and accuracy of your model.\n",
    "\n",
    "In the end please upload your jupyter notebook to moodle.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418e17a1",
   "metadata": {
    "id": "418e17a1"
   },
   "source": [
    "# If you run the notebook in colab, you have to mount the google drive with the images. Proceed as follows:\n",
    "\n",
    "- **First**: Open the following **[link](https://drive.google.com/drive/folders/1eCU34ZatAXQwzkzHMpV4i2gwlR6qvqpC?usp=share_link)** in a new tab.\n",
    "- **Second**: Add a link to your google Drive.\n",
    "Example: [Link](https://drive.google.com/file/d/1IcFGGIoktPkDj9-4j5IQ3evInn0c2aq-/view?usp=sharing)\n",
    "- **Third**: Run the line of code below\n",
    "- **Fourth**: Grant Google access to your Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6WoxIKCNs6w8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6WoxIKCNs6w8",
    "outputId": "5c7af97e-0e63-4ea9-fafe-65de5fc5ef92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "# path to the link you created\n",
    "path_to_slides = '/content/gdrive/MyDrive/AgNORs/'\n",
    "# mount the data\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ivwojB899YXw",
   "metadata": {
    "id": "ivwojB899YXw"
   },
   "source": [
    "#1. Implement a Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gorgeous-responsibility",
   "metadata": {
    "id": "gorgeous-responsibility"
   },
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, annotations_frame, path_to_slides,  num_samples=1000,crop_size=(128,128), transformations= None):\n",
    "        super().__init__()\n",
    "        self.anno_frame = annotations_frame\n",
    "        self.path_to_slides = path_to_slides\n",
    "        self.crop_size = crop_size\n",
    "        self.num_samples = num_samples\n",
    "        self.transformations = transformations\n",
    "        #self.df = pickle.load(open(self.annotations_frame, 'rb'))\n",
    "        self.images = {}\n",
    "        self._initialize()\n",
    "        self.sample_cord_list = self._sample_cord_list()\n",
    "\n",
    "        \n",
    "    def _initialize(self):\n",
    "        for filename in self.anno_frame['filename'].unique():\n",
    "            img_path = f'{self.path_to_slides}/{filename}'\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "            self.images[filename] = img\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        slide=self.sampled[idx][0]\n",
    "        max_x,max_y,min_x,min_y = self.sampled[idx][1:5]\n",
    "        init_img=self.images[slide].crop((min_x,min_y,max_x,max_y))\n",
    "        label = self.sampled[idx][5]\n",
    "        img = A.Compose([\n",
    "            #A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "            A.Resize(30,30),\n",
    "            ToTensorV2()\n",
    "            ])(image=np.asarray(init_img,dtype=np.float32))\n",
    "        return img['image'],label\n",
    "        \n",
    "    def _sample_cord_list(self):\n",
    "        #stratified sampling and so we need to make a dict of available choices with weights(inverse to frequency)\n",
    "        weights = 1/self.anno_frame['label'].value_counts(normalize=True)\n",
    "        hi=dict(weights)\n",
    "        self.anno_frame['weights'] = self.anno_frame.apply(lambda row:hi[row.label],\n",
    "                                                          axis=1)\n",
    "        selected=self.anno_frame.loc[self.anno_frame['label']>0]\n",
    "        sampled=selected.sample(n=num_samples,weights = 'weights').to_numpy()\n",
    "        self.sampled=sampled \n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def trigger_sampling(self):\n",
    "        self.sample_cord_list = self._sample_cord_list()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WeAfxZSy6N6N",
   "metadata": {
    "id": "WeAfxZSy6N6N"
   },
   "source": [
    "#2. Develop a Custom Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regional-acting",
   "metadata": {
    "id": "regional-acting"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1= nn.Conv2d(3,32,kernel_size = (3,3),stride = 1, padding = 1)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.drop1 = nn.Dropout(0.3)\n",
    "        \n",
    "        self.conv2= nn.Conv2d(32,32,kernel_size = (3,3),stride = 1, padding = 1)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size = (2,2))\n",
    "\n",
    "        self.conv3= nn.Conv2d(32,32,kernel_size = (3,3),stride = 1, padding = 1)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.pool3 = nn.AvgPool2d(kernel_size=(2,2))\n",
    "\n",
    "        self.flat = nn.Flatten()\n",
    "\n",
    "        self.fc3 = nn.Linear(1568,512)\n",
    "        self.act3 = nn.ReLU()\n",
    "        self.drop3 = nn.Dropout(0.3)\n",
    "\n",
    "        self.fc4 = nn.Linear(512,12)\n",
    "        #self.softmax = nn.Softmax(dim=0)\n",
    "\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.act1(self.conv1(x))\n",
    "        x = self.drop1(x)\n",
    "        x = self.act2(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = self.act3(self.conv3(x))\n",
    "        x = self.pool3(x)\n",
    "        x = self.flat(x)\n",
    "        x = self.act3(self.fc3(x))\n",
    "        x = self.drop3(x)\n",
    "        x = self.fc4(x)\n",
    "       # x= self.softmax(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Vl6b7v3_6Ta2",
   "metadata": {
    "id": "Vl6b7v3_6Ta2"
   },
   "source": [
    "#3. Set up Training and Validation/Test Loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "innocent-aggregate",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "innocent-aggregate"
   },
   "outputs": [],
   "source": [
    "def train(dataloader, model, optimizer,criterion):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    #print(\"The model will be running on\", device, \"device\")\n",
    "\n",
    "    running_loss = 0\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    # switch to train mode\n",
    "    if not model.training:\n",
    "        model.train()\n",
    "    size = len(dataloader.dataset)\n",
    "    for epoch in range(5):\n",
    "        for i, data in enumerate(dataloader, 0):\n",
    "        # get the inputs; data is a list of [image, labels]\n",
    "            inputs, labels = data\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            '''if i % 1000 == 0:    # print every 1000 mini-batches/samples\n",
    "                print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "                running_loss = 0.0'''\n",
    "        #print(running_loss/len(dataloader))\n",
    "        running_loss=0.0\n",
    "\n",
    "def validate(dataloader,model):\n",
    "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "  running_loss = 0.0\n",
    "\n",
    "  metric = torchmetrics.classification.MulticlassF1Score(num_classes=12)\n",
    "\n",
    "  \n",
    "  #preds = []\n",
    "  #tag = []\n",
    "\n",
    "  # switch to validation mode\n",
    "  if model.training:\n",
    "      model.eval()\n",
    "\n",
    "  with torch.no_grad():\n",
    "          # iterating over batches in valoader_loader\n",
    "      for i, (inputs,targets) in enumerate(dataloader):\n",
    "          predictions = model(inputs)\n",
    "          metric.update(predictions,targets)\n",
    "          #preds.append(predictions)\n",
    "          #tag.append(targets)\n",
    "  #print(preds)\n",
    "  #print(f\"\\n{tag}\")        \n",
    "  #return preds, tag\n",
    "  metrics_values = metric.compute()\n",
    "    \n",
    "  print(f\"F1 SCORE = {metrics_values}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "innocent-anxiety",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "innocent-anxiety"
   },
   "outputs": [],
   "source": [
    "anno_frame = pickle.load(open('/content/gdrive/MyDrive/AgNORs/annotation_frame.p','rb'))\n",
    "num_samples=1000\n",
    "batch_size=25\n",
    "num_workers=2\n",
    "ds = Dataset(annotations_frame = anno_frame, path_to_slides =path_to_slides,num_samples=num_samples)\n",
    "#img,label = ds[4]\n",
    "#plt.imshow(img.permute(1, 2, 0))\n",
    "train_size = int(0.8 * len(ds))\n",
    "test_size = len(ds) - train_size\n",
    "train_ds, val_ds = torch.utils.data.random_split(ds, [train_size, test_size])\n",
    "\n",
    "#NOW TO CONVERT INTO DATALOADERS\n",
    "train_dl = DataLoader(train_ds,\n",
    "                      batch_size = batch_size,\n",
    "                      num_workers = num_workers)\n",
    "val_dl = DataLoader(val_ds,\n",
    "                    batch_size = batch_size,\n",
    "                    num_workers = num_workers)\n",
    "\n",
    "del train_ds\n",
    "del val_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xIrT1Qsg6ben",
   "metadata": {
    "id": "xIrT1Qsg6ben"
   },
   "source": [
    "#4. Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "golden-delta",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "golden-delta",
    "outputId": "2357231a-a934-4170-a9da-3e89310e13ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.481622904539108\n",
      "2.489897236227989\n",
      "2.3363236859440804\n",
      "2.3056613579392433\n",
      "2.273478128015995\n",
      "tensor(0.0529)\n",
      "2.2498397678136826\n",
      "2.2105274498462677\n",
      "2.1967510879039764\n",
      "2.1826886497437954\n",
      "2.1577973030507565\n",
      "tensor(0.0735)\n",
      "2.173382867127657\n",
      "2.1480185240507126\n",
      "2.0979850627481937\n",
      "2.066579531878233\n",
      "2.047785021364689\n",
      "tensor(0.0780)\n",
      "2.1170380786061287\n",
      "2.0651989690959454\n",
      "2.0385677628219128\n",
      "2.003222245723009\n",
      "1.9738688953220844\n",
      "tensor(0.0908)\n",
      "2.0035055726766586\n",
      "1.9462873749434948\n",
      "1.9116467870771885\n",
      "1.8860482089221478\n",
      "1.8666019290685654\n",
      "tensor(0.0521)\n",
      "1.9401476085186005\n",
      "1.8906584940850735\n",
      "1.8641328029334545\n",
      "1.8224720321595669\n",
      "1.7682668082416058\n",
      "tensor(0.1149)\n",
      "1.9069696366786957\n",
      "1.8046339005231857\n",
      "1.7856644093990326\n",
      "1.7312464341521263\n",
      "1.690734263509512\n",
      "tensor(0.2310)\n",
      "1.8423873595893383\n",
      "1.7411862835288048\n",
      "1.650149330496788\n",
      "1.5885520689189434\n",
      "1.5309211201965809\n",
      "tensor(0.1054)\n",
      "1.7004091553390026\n",
      "1.6174452230334282\n",
      "1.5181778706610203\n",
      "1.4596997424960136\n",
      "1.3926620967686176\n",
      "tensor(0.2526)\n",
      "1.6508149206638336\n",
      "1.5610858760774136\n",
      "1.4296727292239666\n",
      "1.3639974668622017\n",
      "1.2793880999088287\n",
      "tensor(0.2282)\n",
      "1.5221746563911438\n",
      "1.4142512083053589\n",
      "1.3422804325819016\n",
      "1.2334565501660109\n",
      "1.0923711769282818\n",
      "tensor(0.3964)\n",
      "1.416980542242527\n",
      "1.3162244372069836\n",
      "1.2342811692506075\n",
      "1.1337249800562859\n",
      "0.9932073950767517\n",
      "tensor(0.5123)\n",
      "1.4079152308404446\n",
      "1.2054009102284908\n",
      "1.114725735038519\n",
      "1.0005553159862757\n",
      "0.9042088687419891\n",
      "tensor(0.5480)\n",
      "1.3896978236734867\n",
      "1.1565847620368004\n",
      "1.0485604740679264\n",
      "0.9043280947953463\n",
      "0.8102037776261568\n",
      "tensor(0.5066)\n",
      "1.2554709259420633\n",
      "1.040844887495041\n",
      "0.8990745786577463\n",
      "0.7984390622004867\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-c3a678a4c06f>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mtrain_dl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrigger_sampling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_dl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-69-67a0dfe6a007>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataloader, model, optimizer, criterion)\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "model =Net()\n",
    "optimizer = Adam(model.parameters(), lr=0.0001)\n",
    "loss=nn.CrossEntropyLoss()\n",
    "epoch = 20\n",
    "for i in range(epoch):\n",
    "    train_dl.dataset.dataset.trigger_sampling()\n",
    "    train(train_dl,model,optimizer,loss)\n",
    "    validate(val_dl,model)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bqVomcwZohqv",
   "metadata": {
    "id": "bqVomcwZohqv"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "interpreter": {
   "hash": "cbc2ccb7899380f8e39344ba6fff5f38db9a0526ef90eff1147cd398f3029ddf"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

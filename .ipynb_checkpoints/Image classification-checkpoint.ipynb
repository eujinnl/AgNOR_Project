{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7VNgmJuv8kc_",
   "metadata": {
    "id": "7VNgmJuv8kc_"
   },
   "source": [
    "# Install all necessary modules here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "limiting-hughes",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JFNtu2mM8sOR",
    "outputId": "48b6fbdb-00cc-4c80-d44e-b61f3c3b00a6"
   },
   "source": [
    "!pip install torchvision\n",
    "!pip install torchmetrics\n",
    "!pip install torch\n",
    "!pip install numpy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "uo9DV6SrYt4j",
   "metadata": {
    "id": "uo9DV6SrYt4j"
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import platform\n",
    "import pickle\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from numpy import random\n",
    "from torch.utils.data import DataLoader\n",
    "import albumentations as A\n",
    "from matplotlib import pyplot as plt\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bf9f74",
   "metadata": {
    "id": "d3bf9f74"
   },
   "source": [
    "# Milestone Four Preparation\n",
    "\n",
    "Today, we will begin preparing for the fourth milestone. The objective of this milestone is to train an object classification model capable of classifying the discovered cells. To successfully complete this milestone, you should follow the steps outlined below:\n",
    "\n",
    "1. Implement a Dataset Class:\n",
    "   - Create a Dataset Class specifically designed to load images from the \"crops\" directory along with their respective labels.\n",
    "   - Considering the significant class imbalance within the dataset, you will have to make use of approprriate sampling and augmentations.\n",
    "   - Resize all crops to the same size.\n",
    "\n",
    "2. Develop a Custom Classification Model:\n",
    "   - Instead of utilizing an existing model from PyTorch, design your own custom model for this task.\n",
    "\n",
    "3. Set up Training and Validation/Test Loops:\n",
    "   - Write a training and validation/test loop for training your model.\n",
    "   - Select an appropriate optimizer and loss function to facilitate effective training.\n",
    "   - Continue training your model until convergence is observed, as indicated by the validation loss.\n",
    "   - Once training is complete, plot the training and test loss to visualize the learning progress.\n",
    "\n",
    "4. Evaluate Model Performance:\n",
    "   - Calculate the f1-score and accuracy of your model.\n",
    "\n",
    "In the end please upload your jupyter notebook to moodle.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418e17a1",
   "metadata": {
    "id": "418e17a1"
   },
   "source": [
    "# If you run the notebook in colab, you have to mount the google drive with the images. Proceed as follows:\n",
    "\n",
    "- **First**: Open the following **[link](https://drive.google.com/drive/folders/1eCU34ZatAXQwzkzHMpV4i2gwlR6qvqpC?usp=share_link)** in a new tab.\n",
    "- **Second**: Add a link to your google Drive.\n",
    "Example: [Link](https://drive.google.com/file/d/1IcFGGIoktPkDj9-4j5IQ3evInn0c2aq-/view?usp=sharing)\n",
    "- **Third**: Run the line of code below\n",
    "- **Fourth**: Grant Google access to your Drive"
   ]
  },
  {
   "cell_type": "raw",
   "id": "valid-scale",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "brYRbF8idhWa",
    "outputId": "ec0724fe-c855-4524-e622-2df6edc43981"
   },
   "source": [
    "from google.colab import drive\n",
    "\n",
    "# path to the link you created\n",
    "path_to_slides = '/content/gdrive/MyDrive/AgNORs/'\n",
    "# mount the data\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "elementary-employee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "anno_frame = pickle.load(open('annotation_frame.p','rb'))\n",
    "weights = 1/anno_frame['label'].value_counts(normalize=True)\n",
    "hi=dict(weights)\n",
    "\n",
    "anno_frame['weights'] = anno_frame.apply(lambda row:hi[row.label],\n",
    "                                                  axis=1)\n",
    "selected=anno_frame.loc[anno_frame['label']>0]\n",
    "sampled=selected.sample(n=1000,weights = 'weights')\n",
    "sampled1=sampled.to_numpy()\n",
    "max_x,max_y,min_x,min_y = sampled1[5][1:5]\n",
    "print(len(hi))\n",
    "\n",
    "#GOOD ENOUGH SAMPLING FUCK IT I GUESS IT WORKS IM NOT QUITE SURE HOW BUT IT DOES  crop size is30 by 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ivwojB899YXw",
   "metadata": {
    "id": "ivwojB899YXw"
   },
   "source": [
    "#1. Implement a Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "gorgeous-responsibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, annotations_frame, path_to_slides,  num_samples=1000,crop_size=(128,128), transformations= None):\n",
    "        super().__init__()\n",
    "        self.anno_frame = annotations_frame\n",
    "        self.path_to_slides = path_to_slides\n",
    "        self.crop_size = crop_size\n",
    "        self.num_samples = num_samples\n",
    "        self.transformations = transformations\n",
    "        #self.df = pickle.load(open(self.annotations_frame, 'rb'))\n",
    "        self.images = {}\n",
    "        self._initialize()\n",
    "        self.sample_cord_list = self._sample_cord_list()\n",
    "\n",
    "        \n",
    "    def _initialize(self):\n",
    "        for filename in self.anno_frame['filename'].unique():\n",
    "            img_path = f'{self.path_to_slides}images/{filename}'\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "            self.images[filename] = img\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        slide=self.sampled[idx][0]\n",
    "        max_x,max_y,min_x,min_y = self.sampled[idx][1:5]\n",
    "        init_img=self.images[slide].crop((min_x,min_y,max_x,max_y))\n",
    "        label = self.sampled[idx][5]\n",
    "        img = A.Compose([\n",
    "            #A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "            A.Resize(30,30),\n",
    "            ToTensorV2()])(image=np.asarray(init_img))\n",
    "        return img['image'],label\n",
    "        \n",
    "        \n",
    "    \n",
    "    def _sample_cord_list(self):\n",
    "        #stratified sampling and so we need to make a dict of available choices with weights(inverse to frequency)\n",
    "        weights = 1/self.anno_frame['label'].value_counts(normalize=True)\n",
    "        hi=dict(weights)\n",
    "        self.anno_frame['weights'] = self.anno_frame.apply(lambda row:hi[row.label],\n",
    "                                                          axis=1)\n",
    "        selected=self.anno_frame.loc[self.anno_frame['label']>0]\n",
    "        sampled=selected.sample(n=num_samples,weights = 'weights').to_numpy()\n",
    "        self.sampled=sampled \n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        '''\n",
    "        # select slides from which to sample an image\n",
    "        slide_names = np.array(list(self.slide_dict.keys()))\n",
    "        slide_indice = random.choice(np.arange(len(slide_names)), size = self.pseudo_epoch_length, replace = True)\n",
    "        slides = slide_names[slide_indice]\n",
    "        # select coordinates from which to load images\n",
    "        # only works if all images have the same size\n",
    "        width,height = self.slide_dict[slides[0]].size\n",
    "        cordinates = random.randint(low = (0,0), high=(width - self.crop_size[0], height - self.crop_size[1]), size = (self.pseudo_epoch_length,2))\n",
    "        return np.concatenate((slides.reshape(-1,1),cordinates), axis = -1)\n",
    "        '''\n",
    "    \n",
    "    def trigger_sampling(self):\n",
    "        self.sample_cord_list = self._sample_cord_list()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WeAfxZSy6N6N",
   "metadata": {
    "id": "WeAfxZSy6N6N"
   },
   "source": [
    "#2. Develop a Custom Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "retained-spain",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Vl6b7v3_6Ta2",
   "metadata": {
    "id": "Vl6b7v3_6Ta2"
   },
   "source": [
    "#3. Set up Training and Validation/Test Loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "innocent-aggregate",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, optimizer,loss):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"The model will be running on\", device, \"device\")\n",
    "\n",
    "    running_loss = 0\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    # switch to train mode\n",
    "    if not model.training:\n",
    "        model.train()\n",
    "    size = len(dataloader.dataset)\n",
    "    for epoch in range(5):\n",
    "        for i, data in enumerate(dataloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % 20 == 0:    # print every 20 mini-batches/samples\n",
    "                print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "                running_loss = 0.0\n",
    "\n",
    "    # print your losses here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "innocent-anxiety",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.ByteTensor\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "anno_frame = pickle.load(open('annotation_frame.p','rb'))\n",
    "pts = ''\n",
    "num_samples=1000\n",
    "batch_size=25\n",
    "num_workers=2\n",
    "ds = Dataset(annotations_frame = anno_frame, path_to_slides =pts,num_samples=num_samples)\n",
    "img,label = ds[4]\n",
    "plt.imshow(img.permute(1, 2, 0))\n",
    "print(img.type())\n",
    "train_size = int(0.8 * len(ds))\n",
    "test_size = len(ds) - train_size\n",
    "train_ds, val_ds = torch.utils.data.random_split(ds, [train_size, test_size])\n",
    "\n",
    "#NOW TO CONVERT INTO DATALOADERS\n",
    "train_dl = DataLoader(train_ds,\n",
    "                      batch_size = batch_size,\n",
    "                      num_workers = num_workers)\n",
    "val_dl = DataLoader(val_ds,\n",
    "                    batch_size = batch_size,\n",
    "                    num_workers = num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hindu-nancy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model will be running on cuda:0 device\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "model =Net()\n",
    "optimizer = Adam(model.parameters(), lr=0.01, betas=(0.9,0.999))\n",
    "loss=nn.CrossEntropyLoss()\n",
    "epoch = 25\n",
    "for i in range(epoch):\n",
    "    train(train_dl,model,optimizer,loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xIrT1Qsg6ben",
   "metadata": {
    "id": "xIrT1Qsg6ben"
   },
   "source": [
    "#4. Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "permanent-container",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(epoch):\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "interpreter": {
   "hash": "cbc2ccb7899380f8e39344ba6fff5f38db9a0526ef90eff1147cd398f3029ddf"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
